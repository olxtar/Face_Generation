{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fc809e8",
   "metadata": {},
   "source": [
    "# Face Generation\n",
    "\n",
    "이번 Udacity 프로젝트에서는 CelebA라는 데이터셋을 사용하여 (최대한 현실적으로 보이는) 새로운 얼굴 이미지를 생성하는 Deep Convolutional GAN을 만들것이다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4640d3c",
   "metadata": {},
   "source": [
    "### Pre-processed Data\n",
    "\n",
    "* 리얼 원본 데이터셋 : http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html\n",
    "* Pre-processed 데이터셋 다운로드 : https://s3.amazonaws.com/video.udacity-data.com/topher/2018/November/5be7eb6f_processed-celeba-small/processed-celeba-small.zip\n",
    "\n",
    "진짜 CelebA 데이터셋은 대략 20만장의 얼굴 사진, 40개의 Binary Annotation이 딸려있는 데이터셋이다. 하지만 우리는 '그냥' 얼굴 사진만 필요하므로 Pre-process된 데이터를 사용한다\n",
    "\n",
    "<br/>\n",
    "\n",
    "$\\rightarrow$ 64x64x3 사이즈로 얼굴부분만 크롭된 Numpy 이미지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "60fd47d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# zip파일 압축해제 코드\n",
    "# !unzip processed_celeba_small.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6413e799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현 디렉토리: C:\\Users\\USER\\Desktop\\Udacity\\Genarate_Faces\n",
      "현 디렉토리내 파일 및 폴더: ['.ipynb_checkpoints', 'data', 'Generate_Faces.ipynb', 'problem_unittests.py', '__pycache__']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "print(f\"현 디렉토리: {os.getcwd()}\") \n",
    "print(f\"현 디렉토리내 파일 및 폴더: {os.listdir()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2115df39",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">[+]</span> 데이터 경로가 다소 이상하게 되어있다. / 그냥 짝대기와 언더바에 주의\n",
    "\n",
    "`data`폴더 $\\rightarrow$ `processed-celeba-small`폴더 $\\rightarrow$ `processed_celeba_small`폴더 $\\rightarrow$ `celeba`폴더"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ce65e72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/processed-celeba-small/processed_celeba_small/celeba\\\\161979.jpg',\n",
       " 'data/processed-celeba-small/processed_celeba_small/celeba\\\\161980.jpg',\n",
       " 'data/processed-celeba-small/processed_celeba_small/celeba\\\\161981.jpg',\n",
       " 'data/processed-celeba-small/processed_celeba_small/celeba\\\\161982.jpg',\n",
       " 'data/processed-celeba-small/processed_celeba_small/celeba\\\\161983.jpg',\n",
       " 'data/processed-celeba-small/processed_celeba_small/celeba\\\\161984.jpg',\n",
       " 'data/processed-celeba-small/processed_celeba_small/celeba\\\\161985.jpg',\n",
       " 'data/processed-celeba-small/processed_celeba_small/celeba\\\\161986.jpg',\n",
       " 'data/processed-celeba-small/processed_celeba_small/celeba\\\\161987.jpg',\n",
       " 'data/processed-celeba-small/processed_celeba_small/celeba\\\\161988.jpg']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 경로 예시 (10개)\n",
    "glob.glob('data/processed-celeba-small/processed_celeba_small/celeba/*')[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "002c2510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 개수: 32601개\n"
     ]
    }
   ],
   "source": [
    "print(f\"데이터 개수: {len(glob.glob('data/processed-celeba-small/processed_celeba_small/celeba/*'))}개\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11768cb8",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "---\n",
    "\n",
    "<br/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7494e42",
   "metadata": {},
   "source": [
    "## 00. Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dab4d3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './data/processed-celeba-small/processed_celeba_small/'\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "\n",
    "import pickle as pkl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import problem_unittests as tests    # Udacity 제공 .py 파일\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6dfc3a2",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "---\n",
    "\n",
    "<br/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75b18d9",
   "metadata": {},
   "source": [
    "## 01. Visualize the CelebA Data\n",
    "\n",
    "CelebA 데이터셋은 원래 20만장의 얼굴이미지이나 64x64x3(RGB) 사이즈로 전처리해둠"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa864a6",
   "metadata": {},
   "source": [
    "**Exercise** : `get_dataloader` function을 만들자\n",
    "\n",
    "1. `image_size` x `image_size` 의 Tensor return\n",
    "2. shuffle and batch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "200a2ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882ca26e",
   "metadata": {},
   "source": [
    "### 1-1. get_dataloader function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c546a87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader(batch_size, image_size, data_dir='./data/processed-celeba-small/processed_celeba_small/'):\n",
    "\n",
    "    transform = transforms.Compose([transforms.Resize(image_size),\n",
    "                                    transforms.ToTensor()])\n",
    "    image_dataset = datasets.ImageFolder(data_dir, transform)\n",
    "    return torch.utils.data.DataLoader(image_dataset, \n",
    "                                        batch_size = batch_size,\n",
    "                                        shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e1a460",
   "metadata": {},
   "source": [
    "### 1-2. Create a DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53401f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "img_size = 32\n",
    "\n",
    "celeba_train_loader = get_dataloader(batch_size, img_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384b972c",
   "metadata": {},
   "source": [
    "### 1-3. Visualize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9ca6f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7b950a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 batch의 데이터 차원: torch.Size([128, 3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "images, _ = next(iter(celeba_train_loader))\n",
    "print(f\"1 batch의 데이터 차원: {images.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "baaf3b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 batch의 128개의 이미지 중 1개만 PIL이미지로 변환\n",
    "to_pil = torchvision.transforms.ToPILImage()\n",
    "images_PIL = to_pil(images[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31b991eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=32x32 at 0x1373104FCA0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images_PIL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec8387c",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "---\n",
    "\n",
    "<br/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c80c4c1",
   "metadata": {},
   "source": [
    "## 02. More Pre-process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447ae81a",
   "metadata": {},
   "source": [
    "Generator의 output은 `tanh`, 즉 Hyperbolic Tangent function으로 activation되므로 pixel value의 범위를 -1부터 1까지로 조정해줄 필요가 있다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99969180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원래 이미지의 pixel 최소값: 0.0117647061124444\n",
      "원래 이미지의 pixel 최대값: 0.9019607901573181\n"
     ]
    }
   ],
   "source": [
    "# 원래 Tensor 이미지 pixel값의 범위는 0~1\n",
    "print(f\"원래 이미지의 pixel 최소값: {images[8].min()}\")\n",
    "print(f\"원래 이미지의 pixel 최대값: {images[8].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5187476",
   "metadata": {},
   "source": [
    "### 2-1. scale function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6323f75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(x, feature_range=(-1,1)):\n",
    "    min, max = feature_range\n",
    "    return x * (max - min) + min"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6caaaf",
   "metadata": {},
   "source": [
    "### 2-2. Check scaled range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "620608f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scale된 이미지의 pixel 최소값: -0.9764705896377563\n",
      "Scale된 이미지의 pixel 최대값: 0.8039215803146362\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "# check scaled range\n",
    "# should be close to -1 to 1\n",
    "img = images[8]\n",
    "scaled_img = scale(img)\n",
    "\n",
    "print(f\"Scale된 이미지의 pixel 최소값: {scaled_img.min()}\")\n",
    "print(f\"Scale된 이미지의 pixel 최대값: {scaled_img.max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5965016e",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "---\n",
    "\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025eca54",
   "metadata": {},
   "source": [
    "## 03. Define the Model\n",
    "\n",
    "GAN은 Discriminator와 Generator가 경쟁하는 구조로 구성되어 있다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b33dd2",
   "metadata": {},
   "source": [
    "### 3-1. Discriminator\n",
    "\n",
    "Discriminator는 단순하게 Convolutional Classifier이다. without maxpooling layer, 또한 복잡한 데이터를 처리하기 위하여 깊은, 즉 Deep network를 구성해야되는데 이때 Normalization이 필요하게 된다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356de743",
   "metadata": {},
   "source": [
    "1. Discriminator의 입력은 `32x32x3` size의 tensor 이미지\n",
    "2. Discriminator의 출력은 single value, 입력 이미지가 Real or Fake인지를 나타내는"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41a80739",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349e5c58",
   "metadata": {},
   "source": [
    "### 3-2. conv function\n",
    "\n",
    "Discriminator를 정의할때 사용하면 편한 `conv` helper function을 만들자\n",
    "\n",
    "batch_normalization layer를 만들기 쉽게 하는 것으로\n",
    "\n",
    "단순히 `conv` function에 아래의 parameter들을 넣어주면 conv + batch_norm layer를 return\n",
    "\n",
    "* `in_channels` : 입력 데이터의 depth\n",
    "* `out_channels` : 출력 데이터의 depth\n",
    "* `kernel_size` : Kernel or Filter size\n",
    "* `stride` : stride\n",
    "* `padding` : padding\n",
    "* `batch_norm` : Batch Normalization layer 생성 여부 boolean값\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "32ef56d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv(in_channels,\n",
    "         out_channels,\n",
    "         kernel_size,\n",
    "         stride=2,\n",
    "         padding=1,\n",
    "         batch_norm=True):\n",
    "    \n",
    "    layers = []\n",
    "    conv_layer = nn.Conv2d(in_channels = in_channels,\n",
    "                           out_channels = out_channels,\n",
    "                           kernel_size = kernel_size,\n",
    "                           stride = stride,\n",
    "                           padding = padding,\n",
    "                           bias = False)\n",
    "    layers.append(conv_layer)\n",
    "    \n",
    "    if batch_norm:\n",
    "        layers.append(nn.BatchNorm2d(out_channels))\n",
    "        \n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc92339",
   "metadata": {},
   "source": [
    "### 3-3. Define Discriminator\n",
    "\n",
    "* Filter size : 4\n",
    "* Stride : 2\n",
    "* Padding : 1\n",
    "* Input image size : 32x32x3\n",
    "* <span style=\"color:red\">[!]</span> 첫번째 conv layer는 batch norm layer 없이\n",
    "\n",
    "---\n",
    "\n",
    "데이터 차원(size) 변화\n",
    "\n",
    "$$\n",
    "S_{output} = \\frac{ S_{input} + 2 \\times padding - S_{filter}}{Stride} + 1\n",
    "$$\n",
    "\n",
    "$$\n",
    "S_{output} = \\frac{ S_{input} + 2 \\times 1 - 4}{2} + 1 = \\frac{S_{input}}{2}\n",
    "$$\n",
    "\n",
    "`32x32x3` $\\rightarrow$ `16x16xconv_dim` $\\rightarrow$ `8x8xconv_dim*2` $\\rightarrow$ `4x4xconv_dim*4` $\\rightarrow$ `2x2xconv_dim*8` $\\rightarrow$ `1`\n",
    "\n",
    "if `conv_dim` = 64,\n",
    "\n",
    "`3072` $\\rightarrow$ `16384` $\\rightarrow$ `8192` $\\rightarrow$ `4096` $\\rightarrow$ `2048` $\\rightarrow$ `1`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "656aa31b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    \n",
    "    def __init__(self, conv_dim):\n",
    "        \"\"\"\n",
    "        param conv_dim: The depth of the first convolutional layer\n",
    "        \"\"\"\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.conv_dim = conv_dim\n",
    "        \n",
    "        self.conv1 = conv(3, conv_dim, 4, batch_norm = False)\n",
    "        self.conv2 = conv(conv_dim, conv_dim*2, 4)\n",
    "        self.conv3 = conv(conv_dim*2, conv_dim*4, 4)\n",
    "        self.conv4 = conv(conv_dim*4, conv_dim*8, 4)\n",
    "        \n",
    "        self.fc = nn.Linear(2*2*conv_dim*8, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        param x: The input to the neural network(discriminator)\n",
    "        \"\"\"\n",
    "        x = F.leaky_relu(self.conv1(x), 0.2)\n",
    "        x = F.leaky_relu(self.conv2(x), 0.2)\n",
    "        x = F.leaky_relu(self.conv3(x), 0.2)\n",
    "        x = F.leaky_relu(self.conv4(x), 0.2)\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.view(-1, self.conv_dim*8*2*2) \n",
    "        out = self.fc(x)\n",
    "        \n",
    "        return out\n",
    "        \n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_discriminator(Discriminator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ded660",
   "metadata": {},
   "source": [
    "### 3-4. Generator\n",
    "\n",
    "Generator는 <span style=\"color:yellowgreen\">Latent vector</span>, 즉 랜덤노이즈 벡터를 받아서 이를 upsample하여 우리의 데이터셋의 이미지 사이즈인 `32x32x3`와 같은 사이즈의 생성이미지를 출력하여야 한다. \n",
    "\n",
    "따라서 <span style=\"color:orange\">Transpose Convolutional Layer</span> with normalization 이용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9ac21f",
   "metadata": {},
   "source": [
    "1. Generator의 입력은 `z_size`의 크기를 가진 Latent vector\n",
    "2. Generator의 출력은 `32x32x3`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e1a900",
   "metadata": {},
   "source": [
    "### 3-5. deconv function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9246aa39",
   "metadata": {},
   "source": [
    "Generator를  정의할때 사용하면 편한 `deconv` helper function을 만들자\n",
    "\n",
    "batch_normalization layer를 만들기 쉽게 하는 것으로\n",
    "\n",
    "단순히 `deconv` function에 아래의 parameter들을 넣어주면 deconv + batch_norm layer를 return\n",
    "\n",
    "* `in_channels` : 입력 데이터의 depth\n",
    "* `out_channels` : 출력 데이터의 depth\n",
    "* `kernel_size` : Kernel or Filter size\n",
    "* `stride` : stride\n",
    "* `padding` : padding\n",
    "* `batch_norm` : Batch Normalization layer 생성 여부 boolean값\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f3f4616",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deconv(in_channels,\n",
    "           out_channels,\n",
    "           kernel_size,\n",
    "           stride=2,\n",
    "           padding=1,\n",
    "           batch_norm=True):\n",
    "    \n",
    "    layers = []\n",
    "    transpose_conv_layer = nn.ConvTranspose2d(in_channels,\n",
    "                                              out_channels,\n",
    "                                              kernel_size,\n",
    "                                              stride,\n",
    "                                              padding,\n",
    "                                              bias=False)\n",
    "    \n",
    "    layers.append(transpose_conv_layer)\n",
    "    \n",
    "    if batch_norm:\n",
    "        layers.append(nn.BatchNorm2d(out_channels))\n",
    "        \n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6a5e8c",
   "metadata": {},
   "source": [
    "### 3-6. Define Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b501e9d",
   "metadata": {},
   "source": [
    "* Filter size : 4\n",
    "* Stride : 2\n",
    "* Padding : 1\n",
    "* Input latent vector size : `z_size` = 100\n",
    "* <span style=\"color:red\">[!]</span> 마지막 deconv layer는 batch norm layer 없이\n",
    "\n",
    "---\n",
    "\n",
    "데이터 차원(size) 변화\n",
    "\n",
    "$$\n",
    "S_{output} = Stride \\times (S_{input}-1) - 2 \\times padding + S_{filter}\n",
    "$$\n",
    "\n",
    "$$\n",
    "S_{output} = 2 \\times (S_{input}-1) - 2 \\times 1 + 4 = 2 \\times S_{input}\n",
    "$$\n",
    "\n",
    "`100` $\\rightarrow$ `2x2xconv_dim*8` $\\rightarrow$ `4x4xconv_dim*4` $\\rightarrow$ `8x8xconv_dim*2` $\\rightarrow$ `16x16xconv_dim` $\\rightarrow$ `32x32x3`\n",
    "\n",
    "if `conv_dim` = 64,\n",
    "\n",
    "`100` $\\rightarrow$ `2048` $\\rightarrow$ `4096` $\\rightarrow$ `8192` $\\rightarrow$ `16384` $\\rightarrow$ `3072`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "745b8e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "class Generator(nn.Module):\n",
    "    \n",
    "    def __init__(self, z_size, conv_dim):\n",
    "        \"\"\"\n",
    "        param z_size: The length of the input latent vector, z\n",
    "        param conv_dim : The depth of the inputs to the *last* transpose convolutional layer \n",
    "        \"\"\"\n",
    "        super(Generator, self).__init__()\n",
    "        #self.z_size = z_size\n",
    "        self.conv_dim = conv_dim\n",
    "        \n",
    "        self.fc = nn.Linear(z_size, 2*2*conv_dim*8)\n",
    "        \n",
    "        self.deconv1 = deconv(conv_dim*8, conv_dim*4, 4)\n",
    "        self.deconv2 = deconv(conv_dim*4, conv_dim*2, 4)\n",
    "        self.deconv3 = deconv(conv_dim*2, conv_dim, 4)\n",
    "        self.deconv4 = deconv(conv_dim, 3, 4, batch_norm = False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        param x: The input to the neural network(Generator)\n",
    "        return: 32x32x3 Tensor image\n",
    "        \"\"\"\n",
    "        \n",
    "        x = self.fc(x)\n",
    "        \n",
    "        # Reshape to (batch_size, depth, height, width)\n",
    "        # Reshape to (128, conv_dim*8, 2, 2)\n",
    "        x = x.view(-1, self.conv_dim*8, 2, 2)  \n",
    "        \n",
    "        x = F.relu(self.deconv1(x))\n",
    "        x = F.relu(self.deconv2(x))\n",
    "        x = F.relu(self.deconv3(x))\n",
    "        \n",
    "        # Last layer -> tanh activation / not relu\n",
    "        x = self.deconv4(x)\n",
    "        out = torch.tanh(x)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_generator(Generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453643c4",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "---\n",
    "\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e38e6d",
   "metadata": {},
   "source": [
    "## 04. Initialize the weights of networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b64aef",
   "metadata": {},
   "source": [
    "### + How to access network's weight\n",
    "network(Discriminator, Generator)의 weight를 조금 더 빨리 수렴할 수 있게끔 임의의 값들로 초기화(initialize)하고자 한다 <span style=\"color:red\">(중앙, 즉 평균이 0이고, 표준편차가 0.02인 정규분포화)</span>\n",
    "\n",
    "$\\rightarrow$ All weights were initialized from a zero-centered Normal distribution with standard deviation 0.02.\n",
    "\n",
    "* This should initialize only **convolutional** and **linear** layers\n",
    "* Initialize the weights to a normal distribution, centered around 0, with a standard deviation of 0.02\n",
    "* THe bias terms, if they exist, may be left alone or set to 0.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d84fac",
   "metadata": {},
   "source": [
    "### 4-1. weights_init_normal function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4dfe7dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init_normal(m):\n",
    "    \"\"\"\n",
    "    Applies initial weights to certain layers in a model.\n",
    "    The weights are taken from a normal distribution with mean = 0, std dev = 0.02\n",
    "    param m: A module or layer in a network\n",
    "    \"\"\"\n",
    "    classname = m.__class__.__name__\n",
    "    \n",
    "    if hasattr(m, 'weight') and (classname.find('Conv') != -1 or classname.find('Linear') != -1):\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "        \n",
    "        # The bias terms, if they exist, set to 0\n",
    "        if hasattr(m, 'bias') and m.bias is not None:\n",
    "            m.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04acdba1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15b95ff",
   "metadata": {},
   "source": [
    "### 4-2. weight_init_normal function test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab3450f",
   "metadata": {},
   "source": [
    "\n",
    "<span style=\"color:blue\">Example</span>\n",
    "\n",
    "\n",
    "아래와 같이 정의된 module(model) `D`에\n",
    "\n",
    "`D = Discriminator(conv_dim = 64)`\n",
    "\n",
    "`D.__class__` : 해당 Instance의 부모 Class를 불러오고\n",
    "\n",
    "`D.__class__.__name__`을 통해 부모 Class의 '이름'들을 불러올 수 있다. 그냥 실행 시`Discriminator`만 나오지만 `apply` function을 통하여 각 Layer들의 이름을 따올 수 있다\n",
    "\n",
    "우리가 만들었던 Class인 `Discriminator`는 4개의 Convolutional, 3개의 BatchNorm, 1개의 Linear Layer들로 이루어져있었다\n",
    "\n",
    "따라서 `D_test.__class__.__name__`을 하나의 변수로 할당해놓고\n",
    "\n",
    "해당 변수에 `.find('Conv' or 'BatchNorm' or 'Linear')`등을 돌리면\n",
    "\n",
    "각각 `'Conv', 'BatchNorm', 'Linear'`일때 -1이 아닌 값을 return하나보다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "753a8f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "D_test = Discriminator(conv_dim = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "410b244f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0578,  0.0527,  0.0778, -0.0025], grad_fn=<SelectBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[-0.0056, -0.0146, -0.0220,  ..., -0.0074,  0.0034, -0.0030]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "## Initiate 이전 weight값 / (0,0,0)부분만 인덱싱하여 보기!\n",
    "# conv1 layer weight \n",
    "print(D_test._modules['conv1']._modules['0'].weight[0,0,0])\n",
    "\n",
    "# fc layer weight\n",
    "print(D_test._modules['fc'].weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1eeec83e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (conv1): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "  )\n",
       "  (conv2): Sequential(\n",
       "    (0): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (conv3): Sequential(\n",
       "    (0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (conv4): Sequential(\n",
       "    (0): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (fc): Linear(in_features=2048, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# weight initiation\n",
    "D_test.apply(weights_init_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "77dafd1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0069, -0.0099, -0.0337, -0.0148], grad_fn=<SelectBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0377, -0.0392, -0.0008,  ..., -0.0120, -0.0164,  0.0213]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "## Initiate 이후 weight값 / (0,0,0)부분만 인덱싱하여 보기!\n",
    "# conv1 layer weight \n",
    "print(D_test._modules['conv1']._modules['0'].weight[0,0,0])\n",
    "\n",
    "# fc layer weight\n",
    "print(D_test._modules['fc'].weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf5a255",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "---\n",
    "\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e34868",
   "metadata": {},
   "source": [
    "## 05. Build complete network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec511574",
   "metadata": {},
   "source": [
    "network model의 hyperparameter를 설정하고\n",
    "\n",
    "앞서 만든 Discriminator, Generator Class를 이용하여 network를 완성시켜보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6a916d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "def build_network(d_conv_dim, g_conv_dim, z_size):\n",
    "    # define discriminator and generator\n",
    "    D = Discriminator(d_conv_dim)\n",
    "    G = Generator(z_size=z_size, conv_dim=g_conv_dim)\n",
    "\n",
    "    # initialize model weights\n",
    "    D.apply(weights_init_normal)\n",
    "    G.apply(weights_init_normal)\n",
    "\n",
    "    print(D)\n",
    "    print()\n",
    "    print(G)\n",
    "    \n",
    "    return D, G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e04d8c80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator(\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  )\n",
      "  (conv2): Sequential(\n",
      "    (0): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (conv3): Sequential(\n",
      "    (0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (conv4): Sequential(\n",
      "    (0): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (fc): Linear(in_features=2048, out_features=1, bias=True)\n",
      ")\n",
      "\n",
      "Generator(\n",
      "  (fc): Linear(in_features=100, out_features=2048, bias=True)\n",
      "  (deconv1): Sequential(\n",
      "    (0): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (deconv2): Sequential(\n",
      "    (0): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (deconv3): Sequential(\n",
      "    (0): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (deconv4): Sequential(\n",
      "    (0): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Define model hyperparams\n",
    "d_conv_dim = 64\n",
    "g_conv_dim = 64\n",
    "z_size = 100\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "D, G = build_network(d_conv_dim, g_conv_dim, z_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dae17e2",
   "metadata": {},
   "source": [
    "### + Traning on GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048dfe9e",
   "metadata": {},
   "source": [
    "`train_on_gpu`라는 boolean 변수를 만들어서 \n",
    "\n",
    "* model\n",
    "* (model) input\n",
    "* loss function argument\n",
    "\n",
    "를 GPU로 이동시키는 것을 까먹지말자!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2813e836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU found. Please use a GPU to train your neural network.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import torch\n",
    "\n",
    "# Check for a GPU\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "if not train_on_gpu:\n",
    "    print('No GPU found. Please use a GPU to train your neural network.')\n",
    "else:\n",
    "    print('Training on GPU!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0abb2a",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "---\n",
    "\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6bce47",
   "metadata": {},
   "source": [
    "## 06. Discriminator and Generator Losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71905f6d",
   "metadata": {},
   "source": [
    "### 6-1. Discriminator Losses\n",
    "\n",
    "* Discriminator의 Losses\n",
    "  1. 진짜 이미지 판별 시의 Loss\n",
    "  2. 가짜 이미지 판별 시의 Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6850c958",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdf8988",
   "metadata": {},
   "source": [
    "### 6-2. Generator Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c08842b",
   "metadata": {},
   "source": [
    "* Generator의 Loss\n",
    "  * Discriminator에 Generator가 생성한 이미지(가짜)를 주면서, 너는 지금 진짜 이미지를 판별하는거야 라고 했을 때의 Loss $\\rightarrow$ <span style=\"color:orange\">flipped label</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a8064e",
   "metadata": {},
   "source": [
    "#### Exercise: Complete real and fake loss functions!\n",
    "\n",
    "You may choose to use either cross entropy or a least squares error loss to complete the following `real_loss` and `fake_loss` functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d6cee3",
   "metadata": {},
   "source": [
    "### 6-3. real_loss and fake_loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "173932f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def real_loss(D_out):\n",
    "    \"\"\"\n",
    "    Discriminator의 출력이 Real에 얼마나 가까운지 계산\n",
    "    param D_out: Discriminator logits\n",
    "    return: real loss값 \n",
    "    \"\"\"\n",
    "    # Make label first!\n",
    "    # 따라서 batch_size를 먼저 파악해야함\n",
    "    batch_size = D_out.size(0)\n",
    "    labels = torch.ones(batch_size) * 0.9 # Label Smoothing\n",
    "    if train_on_gpu:\n",
    "        labels = labels.cuda()\n",
    "    \n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    loss = criterion(D_out.squeeze(), labels)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def fake_loss(D_out):\n",
    "    \"\"\"\n",
    "    Discriminator의 출력이 Fake에 얼마나 가까운지 계산\n",
    "    param D_out: Discriminator logits\n",
    "    return: fake loss값\n",
    "    \"\"\"\n",
    "    batch_size = D_out.size(0)\n",
    "    labels = torch.zeros(batch_size)\n",
    "    if train_on_gpu:\n",
    "        labels = labels.cuda()\n",
    "        \n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    loss = criterion(D_out.squeeze(), labels)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bb29b5",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "---\n",
    "\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dfadc90",
   "metadata": {},
   "source": [
    "## 07. Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b896cc",
   "metadata": {},
   "source": [
    "Discriminator(D)와 Generator(G)를 위한 optimizer를 정의하자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d4846826",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "lr = 0.0002\n",
    "beta1 = 0.5\n",
    "beta2 = 0.999\n",
    "\n",
    "d_optimizer = optim.Adam(D.parameters(), lr, [beta1, beta2])\n",
    "g_optimizer = optim.Adam(G.parameters(), lr, [beta1, beta2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534355a7",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "---\n",
    "\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe3c912",
   "metadata": {},
   "source": [
    "## 08. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26603b8e",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">**[+]**</span> Saving Samples\n",
    "\n",
    "`fixed_z`, 즉 고정된 Latent vector를 통해서 훈련이 진행되는 동안 Generator의 발전 경과를 지켜보기 위하여 Sample을 만들고 저장하자\n",
    "\n",
    "\n",
    "<span style=\"color:green\">**[+]**</span> Move to GPU (model, inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed13550",
   "metadata": {},
   "source": [
    "### 8-1. train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "985d8e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b8d48d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(D, G, n_epochs, print_every=50):\n",
    "    \"\"\"\n",
    "    param D: the discriminator network\n",
    "    param G: the generator network\n",
    "    param n_epochs: the number of epochs to train for\n",
    "    param print_every: when to print and record the model's losses\n",
    "    return: D and G losses\n",
    "    \"\"\"\n",
    "    \n",
    "    # Move models to GPU\n",
    "    if train_on_gpu:\n",
    "        D.cuda()\n",
    "        G.cuda()\n",
    "        \n",
    "    # Loss와 Sample을 keep tracking할 리스트\n",
    "    samples = []\n",
    "    losses = []\n",
    "    \n",
    "    # Sample생성을 위한 fixed data (constant throughout training)\n",
    "    sample_size = 16\n",
    "    fixed_z = np.random.uniform(-1, 1, size=(sample_size, z_size))   # -1~1값\n",
    "    fixed_z = torch.from_numpy(fixed_z).float()\n",
    "    if train_on_gpu:\n",
    "        fixed_z = fixed_z.cuda()\n",
    "        \n",
    "    ###################################################################\n",
    "    \n",
    "    # epoch training loop\n",
    "    for epoch in range(n_epochs):\n",
    "        start = time()\n",
    "        \n",
    "        # batch training loop\n",
    "        for batch_i, (real_images, _) in enumerate(celeba_train_loader):\n",
    "            \n",
    "            batch_size = real_images.size(0)\n",
    "            # Scale the input data(Real images)\n",
    "            real_images = scale(real_images)\n",
    "            \n",
    "            #######################\n",
    "            # TRAIN DISCRIMINATOR #\n",
    "            #######################\n",
    "            d_optimizer.zero_grad()\n",
    "\n",
    "                \n",
    "            # 1. Real loss\n",
    "            if train_on_gpu:\n",
    "                real_images = real_images.cuda()\n",
    "            \n",
    "            D_real = D(real_images)         # Discriminator <- Real images\n",
    "            d_real_loss = real_loss(D_real) # Discriminator Real loss\n",
    "            \n",
    "            \n",
    "            # 2. Fake loss\n",
    "            z = np.random.uniform(-1, 1, size = (batch_size, z_size))  \n",
    "            z = torch.from_numpy(z).float() # Latent Vector 생성\n",
    "            if train_on_gpu:\n",
    "                z = z.cuda()\n",
    "            fake_images = G(z)              # Fake image 생성\n",
    "            \n",
    "            D_fake = D(fake_images)\n",
    "            d_fake_loss = fake_loss(D_fake)\n",
    "            \n",
    "            # 3. Loss sum and Backpropagation\n",
    "            d_loss = d_real_loss + d_fake_loss\n",
    "            d_loss.backward()\n",
    "            d_optimizer.step()\n",
    "            \n",
    "            \n",
    "            ###################\n",
    "            # TRAIN GENERATOR #\n",
    "            ###################\n",
    "            g_optimizer.zero_grad()\n",
    "            \n",
    "            z = np.random.uniform(-1, 1, size = (batch_size, z_size))\n",
    "            z = torch.from_numpy(z).float()  # Latent Vector 생성\n",
    "            if train_on_gpu:\n",
    "                z = z.cuda()\n",
    "            fake_images = G(z)               # Fake image 생성\n",
    "            \n",
    "            D_fake = D(fake_images)\n",
    "            g_loss = real_loss(D_fake)       # Flipped label\n",
    "            \n",
    "            \n",
    "            g_loss.backward()\n",
    "            g_optimizer.step()\n",
    "\n",
    "            # ===============================================\n",
    "            #              END OF YOUR CODE\n",
    "            # ===============================================\n",
    "\n",
    "            # Print some loss stats\n",
    "            if batch_i % print_every == 0:\n",
    "                # append discriminator loss and generator loss\n",
    "                losses.append((d_loss.item(), g_loss.item()))\n",
    "                # print discriminator and generator loss\n",
    "                print('Epoch [{:5d}/{:5d}] | d_loss: {:6.4f} | g_loss: {:6.4f}'.format(\n",
    "                        epoch+1, n_epochs, d_loss.item(), g_loss.item()))\n",
    "\n",
    "\n",
    "        ## AFTER EACH EPOCH## \n",
    "        end = time()\n",
    "        print(f\"Epoch spending time: {end-start}[sec]\")\n",
    "        # this code assumes your generator is named G, feel free to change the name\n",
    "        # generate and save sample, fake images\n",
    "        G.eval() # for generating samples\n",
    "        samples_z = G(fixed_z)\n",
    "        samples.append(samples_z)\n",
    "        G.train() # back to training mode\n",
    "\n",
    "    # Save training generator samples\n",
    "    with open('train_samples.pkl', 'wb') as f:\n",
    "        pkl.dump(samples, f)\n",
    "    \n",
    "    # finally return losses\n",
    "    return losses\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b998ea78",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b97c8ad",
   "metadata": {},
   "source": [
    "### 8-2. Let's Train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "edabc183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [    1/    5] | d_loss: 0.6976 | g_loss: 2.3783\n",
      "Epoch [    1/    5] | d_loss: 0.8793 | g_loss: 1.4895\n",
      "Epoch [    1/    5] | d_loss: 0.7453 | g_loss: 2.0885\n",
      "Epoch [    1/    5] | d_loss: 0.9055 | g_loss: 1.4087\n",
      "Epoch [    1/    5] | d_loss: 0.8046 | g_loss: 2.2764\n",
      "Epoch [    1/    5] | d_loss: 0.7955 | g_loss: 2.1212\n",
      "Epoch [    1/    5] | d_loss: 0.8742 | g_loss: 2.7672\n",
      "Epoch [    1/    5] | d_loss: 0.7508 | g_loss: 1.7580\n",
      "Epoch [    1/    5] | d_loss: 0.8972 | g_loss: 1.7392\n",
      "Epoch [    1/    5] | d_loss: 1.2865 | g_loss: 3.4747\n",
      "Epoch [    1/    5] | d_loss: 1.0844 | g_loss: 1.0743\n",
      "Epoch [    1/    5] | d_loss: 0.9307 | g_loss: 2.6171\n",
      "Epoch [    1/    5] | d_loss: 0.9831 | g_loss: 1.3340\n",
      "Epoch [    1/    5] | d_loss: 0.9195 | g_loss: 1.3072\n",
      "Epoch [    1/    5] | d_loss: 0.8479 | g_loss: 2.4017\n",
      "Epoch spending time: 1094.4432027339935[sec]\n",
      "Epoch [    2/    5] | d_loss: 0.9538 | g_loss: 2.2382\n",
      "Epoch [    2/    5] | d_loss: 0.7901 | g_loss: 2.7951\n",
      "Epoch [    2/    5] | d_loss: 0.9681 | g_loss: 2.6364\n",
      "Epoch [    2/    5] | d_loss: 0.8771 | g_loss: 1.4196\n",
      "Epoch [    2/    5] | d_loss: 0.9699 | g_loss: 1.6202\n",
      "Epoch [    2/    5] | d_loss: 1.0877 | g_loss: 1.4922\n",
      "Epoch [    2/    5] | d_loss: 0.9778 | g_loss: 2.4719\n",
      "Epoch [    2/    5] | d_loss: 0.9729 | g_loss: 1.2632\n",
      "Epoch [    2/    5] | d_loss: 0.9968 | g_loss: 1.4357\n",
      "Epoch [    2/    5] | d_loss: 0.9251 | g_loss: 1.1445\n",
      "Epoch [    2/    5] | d_loss: 0.8235 | g_loss: 1.9465\n",
      "Epoch [    2/    5] | d_loss: 0.8572 | g_loss: 1.5795\n",
      "Epoch [    2/    5] | d_loss: 0.8291 | g_loss: 2.2272\n",
      "Epoch [    2/    5] | d_loss: 0.9023 | g_loss: 2.1049\n",
      "Epoch [    2/    5] | d_loss: 0.8711 | g_loss: 1.7033\n",
      "Epoch spending time: 1082.3993582725525[sec]\n",
      "Epoch [    3/    5] | d_loss: 0.7140 | g_loss: 1.9055\n",
      "Epoch [    3/    5] | d_loss: 1.1357 | g_loss: 0.8314\n",
      "Epoch [    3/    5] | d_loss: 0.9684 | g_loss: 2.1675\n",
      "Epoch [    3/    5] | d_loss: 0.8957 | g_loss: 2.0980\n",
      "Epoch [    3/    5] | d_loss: 0.8586 | g_loss: 2.1536\n",
      "Epoch [    3/    5] | d_loss: 0.8664 | g_loss: 2.8153\n",
      "Epoch [    3/    5] | d_loss: 0.8470 | g_loss: 2.2220\n",
      "Epoch [    3/    5] | d_loss: 0.9538 | g_loss: 1.2809\n",
      "Epoch [    3/    5] | d_loss: 0.7422 | g_loss: 1.8647\n",
      "Epoch [    3/    5] | d_loss: 1.3591 | g_loss: 2.8740\n",
      "Epoch [    3/    5] | d_loss: 0.9813 | g_loss: 1.6411\n",
      "Epoch [    3/    5] | d_loss: 0.8259 | g_loss: 1.3568\n",
      "Epoch [    3/    5] | d_loss: 1.0311 | g_loss: 2.7534\n",
      "Epoch [    3/    5] | d_loss: 0.6555 | g_loss: 2.1606\n",
      "Epoch [    3/    5] | d_loss: 0.8995 | g_loss: 2.1875\n",
      "Epoch spending time: 1088.9513421058655[sec]\n",
      "Epoch [    4/    5] | d_loss: 0.9863 | g_loss: 1.3209\n",
      "Epoch [    4/    5] | d_loss: 0.8816 | g_loss: 2.7433\n",
      "Epoch [    4/    5] | d_loss: 0.7685 | g_loss: 2.2059\n",
      "Epoch [    4/    5] | d_loss: 1.3969 | g_loss: 1.1431\n",
      "Epoch [    4/    5] | d_loss: 0.7887 | g_loss: 1.9237\n",
      "Epoch [    4/    5] | d_loss: 1.0662 | g_loss: 3.3676\n",
      "Epoch [    4/    5] | d_loss: 0.7646 | g_loss: 2.4781\n",
      "Epoch [    4/    5] | d_loss: 0.7638 | g_loss: 1.5044\n",
      "Epoch [    4/    5] | d_loss: 0.7011 | g_loss: 2.1086\n",
      "Epoch [    4/    5] | d_loss: 0.8314 | g_loss: 2.6659\n",
      "Epoch [    4/    5] | d_loss: 0.8256 | g_loss: 2.0772\n",
      "Epoch [    4/    5] | d_loss: 1.0111 | g_loss: 1.3409\n",
      "Epoch [    4/    5] | d_loss: 0.9593 | g_loss: 3.4067\n",
      "Epoch [    4/    5] | d_loss: 1.0431 | g_loss: 1.5198\n",
      "Epoch [    4/    5] | d_loss: 0.8413 | g_loss: 1.6405\n",
      "Epoch spending time: 1076.6291756629944[sec]\n",
      "Epoch [    5/    5] | d_loss: 0.8937 | g_loss: 2.5987\n",
      "Epoch [    5/    5] | d_loss: 1.0825 | g_loss: 1.4146\n",
      "Epoch [    5/    5] | d_loss: 0.9450 | g_loss: 1.7205\n",
      "Epoch [    5/    5] | d_loss: 0.6679 | g_loss: 3.0372\n",
      "Epoch [    5/    5] | d_loss: 0.7541 | g_loss: 1.7662\n",
      "Epoch [    5/    5] | d_loss: 0.8176 | g_loss: 1.9458\n",
      "Epoch [    5/    5] | d_loss: 0.8310 | g_loss: 2.2811\n",
      "Epoch [    5/    5] | d_loss: 0.7518 | g_loss: 1.9801\n",
      "Epoch [    5/    5] | d_loss: 0.8140 | g_loss: 1.6574\n",
      "Epoch [    5/    5] | d_loss: 0.7700 | g_loss: 1.6729\n",
      "Epoch [    5/    5] | d_loss: 0.6973 | g_loss: 2.8579\n",
      "Epoch [    5/    5] | d_loss: 0.6869 | g_loss: 1.6751\n",
      "Epoch [    5/    5] | d_loss: 0.8760 | g_loss: 2.0127\n",
      "Epoch [    5/    5] | d_loss: 0.8298 | g_loss: 2.2286\n",
      "Epoch [    5/    5] | d_loss: 0.8764 | g_loss: 2.0549\n",
      "Epoch spending time: 1011.5896284580231[sec]\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 5\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# call training function\n",
    "losses = train(D, G, n_epochs=n_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec102e5",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "---\n",
    "\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b53b71b",
   "metadata": {},
   "source": [
    "## 09. Training Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed82020",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "losses = np.array(losses)\n",
    "plt.plot(losses.T[0], label='Discriminator', alpha=0.5)\n",
    "plt.plot(losses.T[1], label='Generator', alpha=0.5)\n",
    "plt.title(\"Training Losses\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6c7bb4",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "---\n",
    "\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e96bf3",
   "metadata": {},
   "source": [
    "## 10. Generator samples from training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9773630a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for viewing a list of passed in sample images\n",
    "def view_samples(epoch, samples):\n",
    "    fig, axes = plt.subplots(figsize=(16,4), nrows=2, ncols=8, sharey=True, sharex=True)\n",
    "    for ax, img in zip(axes.flatten(), samples[epoch]):\n",
    "        img = img.detach().cpu().numpy()\n",
    "        img = np.transpose(img, (1, 2, 0))\n",
    "        img = ((img + 1)*255 / (2)).astype(np.uint8)\n",
    "        ax.xaxis.set_visible(False)\n",
    "        ax.yaxis.set_visible(False)\n",
    "        im = ax.imshow(img.reshape((32,32,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8eab2a03",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pkl' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2380/464879215.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Load samples from generator, taken while training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'train_samples.pkl'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0msamples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpkl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'pkl' is not defined"
     ]
    }
   ],
   "source": [
    "# Load samples from generator, taken while training\n",
    "with open('train_samples.pkl', 'rb') as f:\n",
    "    samples = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1ad421",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = view_samples(-1, samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3b451a",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "---\n",
    "\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8781edc5",
   "metadata": {},
   "source": [
    "### Question: What do you notice about your generated samples and how might you improve this model?\n",
    "\n",
    "When you answer this question, consider the following factors:\n",
    "* The dataset is biased; it is made of \"celebrity\" faces that are mostly white\n",
    "* Model size; larger models have the opportunity to learn more features in a data feature space\n",
    "* Optimization strategy; optimizers and number of epochs affect your final result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22f5468",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
